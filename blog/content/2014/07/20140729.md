Title: 2014-07-29
Date: 2014-07-30 22:30
Category: 2014
Tags: 201407, hadoop
Author: laomie
Summary: hadoop安装

安装必要的软件
-----------------
安装jdk和openssh并将hadoop解压到相关路径，此处为"/home/hduser/tools/hadoop"

设置用户和组
----------------------
增加"hadoop"组和"hduser“用户
```bash
sudo groupadd hadoop
sudo adduser --ingroup hadoop hduser
(archlinux: sudo seradd -m -g hadoop -s /bin/bash hduser)
sudo passwd hduser
```

ssh设置
----------------------
```bash
su - hduser
ssh-keygen -t rsa -P ""
cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
```

禁用ipv6
----------------------
在"/etc/sysctl.conf"加入以下内容（注：archlinux为"/etc/sysctl.d/99-sysctl.conf"）
```
#disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
```

环境变量设置
---------------------
在"hduser“用户路径下的".bashrc”里加入以下内容
```bash
# Set Hadoop-related environment variables
export HADOOP_PREFIX=/home/hduser/tools/hadoop
export HADOOP_HOME=/home/hduser/tools/hadoop
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export YARN_HOME=${HADOOP_HOME}
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
# Native Path
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_PREFIX/lib"
export JAVA_LIBRARY_PATH=${HADOOP_PREFIX}/lib/native 
#Java path
export JAVA_HOME='/home/hduser/tools/jdk7'
# Add Hadoop bin/ directory to PATH
export PATH=$PATH:$HADOOP_HOME/bin:$JAVA_PATH/bin:$HADOOP_HOME/sbin
```

建hadoop的数据目录和临时目录
-------------------
```bash
source ~/.bashrc
mkdir -p $HADOOP_HOME/yarn_data/hdfs/namenode
mkdir -p $HADOOP_HOME/yarn_data/hdfs/datanode
mkdir -p $HADOOP_HOME/tmp
```

hadoop配置文件设置
--------------------
1\. 为"libexec/hadoop-config.sh"设置"JAVA_HOME"
```bash
export JAVA_HOME='/home/hduser/tools/jdk7'
```

2\. 为"etc/hadoop/hadoop-env.sh"设置"JAVA_HOME"
```bash
export JAVA_HOME='/home/hduser/tools/jdk7'
```

3\. 设置"etc/hadoop/yarn-site.xml"
```xml
<configuration>
    <!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <!-- "laomie-pc" is master node hostname -->
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>laomie-pc:8025</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>laomie-pc:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>laomie-pc:8040</value>
    </property>

</configuration>
```

4\. 设置"etc/hadoop/core-site.xml"
```xml
<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hduser/tools/hadoop/tmp</value>
    </property>
    <property>
        <name>hadoop.native.lib</name>
        <value>true</value>
    </property>
</configuration>
```

5\. 设置"etc/hadoop/mapred-site.xml" （注：mapred-site.xml.template生成mapred-site.xml）
```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

6\. 设置"etc/hadoop/hdfs-site.xml"
```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <!-- the number of nodes -->
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/home/hduser/tools/hadoop/yarn_data/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/home/hduser/tools/hadoop/yarn_data/hdfs/datanode</value>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
</configuration>
```

hadoop名称节点格式化
---------------------
```bash
hdfs namenode -format
```

hadoop的启动，停止
--------------------
```bash
start-dfs.sh
start-yarn.sh
stop-yarn.sh
stop-dfs.sh
```

hadoop服务
-------------------
* namenode (50070)

* cluster and applications  (8088)

* secondary namenode  (50090)

* datanode (50075)

多节点hadoop设置
----------------------
在"/etc/hosts"设置主从节点机
```
192.168.1.12  laomie-pc
192.168.1.11  slave01
```

主节点机"laomie-pc"ssh登录从节点机"slave01"的设置（注：所有从节点机都需设置）
```bash
ssh-copy-id -i /home/hduser/.ssh/id_rsa.pub hduser@slave01
```

在主节点机的hadoop配置"$HADOOP_CONF_DIR/slaves"中添加从节点机
```
slave01
```

hadoop的代码编译 (archlinux)
--------------------
安装下列依赖软件
```bash
sudo pacman -S cmake apache-ant base-devel protobuf maven
ln -s /etc/profile.d/jre.sh /etc/profile.d/jdk.sh
sudo yaourt -S findbugs
```

修改~/.bashrc
```bash
export FINDBUGS_HOME=/opt/findbugs
export PATH=$PATH:$FINDBUGS_HOME/bin
```

编译hadoop
```bash
mvn package -Pdist,native,docs,src -DskipTests -Dtar
```

hadoop的代码编译 (ubuntu)
--------------------
安装下列依赖软件
```bash
$ sudo add-apt-repository ppa:webupd8team/java
$ sudo apt-get --yes install openjdk-7-jdk subversion pkg-config libssl-dev git ant maven cmake build-essential zlib1g-dev lib32z1-dev libsnappy-dev oracle-java8-installer
```

安装protobuf2.5.0
```bash
$ ./configure
$ make
$ make check
$ sudo make install
$ sudo ldconfig
```

安装findbugs3.0.0
* 解压并如上设置环境变量
 
jdk8编译hadoop （注：不能编译doc）
```bash
mvn package -Pdist,native -DskipTests -Dtar -Dmaven.javadoc.skip=true 
```

references
-----------------------------------------------
* <http://www.cnblogs.com/lanxuezaipiao/p/3525554.html>
* <http://solaimurugan.blogspot.com/2013/11/setup-multi-node-hadoop-20-cluster.html>
